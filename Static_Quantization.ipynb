{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage():\n",
    "    process = psutil.Process()\n",
    "    mem_info = process.memory_info()\n",
    "    return mem_info.rss / (1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before loading model: 355.29 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9623a4f0fa2e400880442a17e91fd0dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after loading model: 26090.16 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Memory usage before loading model: {get_memory_usage():.2f} MB\")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Memory usage after loading model: {get_memory_usage():.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before loading model: 0.00 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ba3243d3084d80b009d009514908da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after loading model: 0.00 MB\n",
      "Model weights memory: 25705.02 MB (GPU)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def get_memory_usage():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.memory_allocated() / (1024 ** 2)  # Convert bytes to MB\n",
    "    else:\n",
    "        # For CPU-based memory usage (using the process size)\n",
    "        import os, psutil\n",
    "        process = psutil.Process(os.getpid())\n",
    "        return process.memory_info().rss / (1024 ** 2)  # Convert bytes to MB\n",
    "\n",
    "# Check memory usage before loading the model\n",
    "print(f\"Memory usage before loading model: {get_memory_usage():.2f} MB\")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Check memory usage after loading the model\n",
    "print(f\"Memory usage after loading model: {get_memory_usage():.2f} MB\")\n",
    "\n",
    "# Check memory usage of model weights (GPU)\n",
    "if torch.cuda.is_available():\n",
    "    model_memory = sum(p.numel() for p in model.parameters() if p.requires_grad) * 4 / (1024 ** 2)  # 4 bytes for float32\n",
    "    print(f\"Model weights memory: {model_memory:.2f} MB (GPU)\")\n",
    "else:\n",
    "    model_memory = sum(p.numel() for p in model.parameters() if p.requires_grad) * 4 / (1024 ** 2)  # 4 bytes for float32\n",
    "    print(f\"Model weights memory: {model_memory:.2f} MB (CPU)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/ao/quantization/quantize.py:312: UserWarning: None of the submodule got qconfig applied. Make sure you passed correct configuration through `qconfig_dict` or by assigning the `.qconfig` attribute directly on submodules\n",
      "  warnings.warn(\"None of the submodule got qconfig applied. Make sure you \"\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define quantization configuration\n",
    "qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "qconfig_dict = {'': qconfig}\n",
    "\n",
    "# Prepare the model for static quantization\n",
    "model_prepared = torch.quantization.prepare(model, qconfig_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration function\n",
    "def calibrate(model_prepared, sample_inputs):\n",
    "    with torch.no_grad():\n",
    "        model_prepared(**sample_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing calibration...\n"
     ]
    }
   ],
   "source": [
    "# Prepare calibration data\n",
    "input_text = \"lion is the king of jungal,\"\n",
    "sample_inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Perform calibration\n",
    "print(\"Performing calibration...\")\n",
    "for _ in range(10):  # Run multiple calibration iterations\n",
    "    calibrate(model, sample_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after static quantization: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Convert the model to quantized version\n",
    "quantized_model = torch.quantization.convert(model_prepared)\n",
    "\n",
    "print(f\"Memory usage after static quantization: {get_memory_usage():.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights memory: 25705.02 MB (GPU)\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model_memory = sum(p.numel() for p in quantized_model.parameters() if p.requires_grad) * 4 / (1024 ** 2)  # 4 bytes for float32\n",
    "    print(f\"Model weights memory: {model_memory:.2f} MB (GPU)\")\n",
    "else:\n",
    "    model_memory = sum(p.numel() for p in quantized_model.parameters() if p.requires_grad) * 4 / (1024 ** 2)  # 4 bytes for float32\n",
    "    print(f\"Model weights memory: {model_memory:.2f} MB (CPU)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lion is the king of jungal, but it is not the king of the world, the king of the world is human.\n",
      "The Lion is the king of the jungle but the king of the world is man.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the quantized model\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**sample_inputs, max_length=50)\n",
    "\n",
    "# Decode the generated tokens\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model sizes\n",
    "def get_model_size(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "    return size_all_mb\n",
    "\n",
    "print(f\"Original model size: {get_model_size(model):.2f} MB\")\n",
    "print(f\"Quantized model size: {get_model_size(quantized_model):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
